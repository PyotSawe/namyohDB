# Query Executor Module Architecture

## Overview
The Query Executor module is responsible for executing physical query plans generated by the optimizer, managing execution resources, and coordinating data flow between operators. It implements a volcano-style iterator model with support for both synchronous and asynchronous execution patterns.

## Core Components

### 1. Execution Engine
The central execution coordinator that manages query execution lifecycle.

```go
type ExecutionEngine interface {
    // Core execution methods
    Execute(ctx context.Context, plan PhysicalPlan, params QueryParameters) (*ResultSet, error)
    ExecuteAsync(ctx context.Context, plan PhysicalPlan, params QueryParameters) (*AsyncResult, error)
    
    // Resource management
    AcquireResources(plan PhysicalPlan) (*ExecutionResources, error)
    ReleaseResources(resources *ExecutionResources) error
    
    // Monitoring and control
    GetExecutionStats(queryID QueryID) (*ExecutionStatistics, error)
    CancelExecution(queryID QueryID) error
    PauseExecution(queryID QueryID) error
    ResumeExecution(queryID QueryID) error
}

type DefaultExecutionEngine struct {
    // Core components
    operatorFactory    *OperatorFactory
    resourceManager    *ResourceManager
    taskScheduler      *TaskScheduler
    memoryManager      *ExecutionMemoryManager
    
    // Execution tracking
    activeExecutions   map[QueryID]*ExecutionContext
    executionStats     *ExecutionStatistics
    
    // Configuration
    config             *ExecutionConfig
    
    // Concurrency control
    maxConcurrentQueries int
    currentExecutions    int32
    executionQueue       chan *ExecutionRequest
    
    mutex                sync.RWMutex
}
```

### 2. Physical Operators
Implementation of physical operators following the iterator model.

#### Base Operator Interface
```go
type PhysicalOperator interface {
    // Iterator interface
    Open(ctx *ExecutionContext) error
    Next() (*Tuple, error)
    Close() error
    
    // Metadata
    Schema() *Schema
    Children() []PhysicalOperator
    OperatorType() OperatorType
    
    // Resource management
    EstimateMemoryUsage() int64
    GetStatistics() *OperatorStatistics
    
    // Execution control
    Reset() error
    SupportsParallelism() bool
    GetParallelism() int
}

// Base operator implementation
type BasePhysicalOperator struct {
    id                 OperatorID
    operatorType       OperatorType
    schema             *Schema
    children           []PhysicalOperator
    context            *ExecutionContext
    statistics         *OperatorStatistics
    memoryUsage        int64
    isOpen             bool
    mutex              sync.RWMutex
}

func (bpo *BasePhysicalOperator) Open(ctx *ExecutionContext) error {
    bpo.mutex.Lock()
    defer bpo.mutex.Unlock()
    
    if bpo.isOpen {
        return fmt.Errorf("operator %s already open", bpo.id)
    }
    
    bpo.context = ctx
    bpo.statistics = NewOperatorStatistics()
    
    // Open all children
    for _, child := range bpo.children {
        if err := child.Open(ctx); err != nil {
            return fmt.Errorf("failed to open child operator: %w", err)
        }
    }
    
    bpo.isOpen = true
    return nil
}
```

#### Scan Operators
```go
// Table scan operator
type TableScanOperator struct {
    BasePhysicalOperator
    
    // Scan configuration
    tableName          string
    filter             Expression
    projection         []ColumnExpression
    
    // Execution state
    iterator           TableIterator
    scanStats          *ScanStatistics
    currentTuple       *Tuple
    
    // Parallel scan support
    scanRanges         []ScanRange
    currentRange       int
    parallelWorkers    int
}

func (tso *TableScanOperator) Open(ctx *ExecutionContext) error {
    if err := tso.BasePhysicalOperator.Open(ctx); err != nil {
        return err
    }
    
    // Acquire table iterator from storage
    storage := ctx.StorageManager
    iterator, err := storage.NewTableIterator(tso.tableName, tso.scanRanges)
    if err != nil {
        return fmt.Errorf("failed to create table iterator: %w", err)
    }
    
    tso.iterator = iterator
    tso.scanStats = NewScanStatistics()
    
    return nil
}

func (tso *TableScanOperator) Next() (*Tuple, error) {
    tso.statistics.IncrementCalls()
    startTime := time.Now()
    defer func() {
        tso.statistics.AddProcessingTime(time.Since(startTime))
    }()
    
    for {
        // Get next tuple from storage
        tuple, err := tso.iterator.Next()
        if err != nil {
            if err == io.EOF {
                return nil, io.EOF
            }
            return nil, fmt.Errorf("table scan error: %w", err)
        }
        
        tso.scanStats.IncrementRowsScanned()
        
        // Apply filter if present
        if tso.filter != nil {
            result, err := tso.filter.Evaluate(tuple)
            if err != nil {
                return nil, fmt.Errorf("filter evaluation error: %w", err)
            }
            
            if !result.(bool) {
                continue // Skip tuple
            }
        }
        
        // Apply projection if specified
        if len(tso.projection) > 0 {
            projectedTuple, err := tso.applyProjection(tuple)
            if err != nil {
                return nil, fmt.Errorf("projection error: %w", err)
            }
            tuple = projectedTuple
        }
        
        tso.statistics.IncrementRowsProduced()
        return tuple, nil
    }
}

// Index scan operator
type IndexScanOperator struct {
    BasePhysicalOperator
    
    // Index configuration
    tableName          string
    indexName          string
    keyConditions      []IndexKeyCondition
    filter             Expression
    scanDirection      ScanDirection
    
    // Execution state
    indexIterator      IndexIterator
    lookupStats        *IndexLookupStatistics
}
```

#### Join Operators
```go
// Hash join operator
type HashJoinOperator struct {
    BasePhysicalOperator
    
    // Join configuration
    leftChild          PhysicalOperator
    rightChild         PhysicalOperator
    joinType           JoinType
    joinCondition      Expression
    leftJoinKeys       []Expression
    rightJoinKeys      []Expression
    
    // Hash table for build side
    hashTable          *HashTable
    buildCompleted     bool
    probePhase         bool
    
    // Execution state
    currentProbeTuple  *Tuple
    matchingBuildTuples []HashTableEntry
    currentMatchIndex   int
    
    // Statistics
    buildStats         *BuildPhaseStatistics
    probeStats         *ProbePhaseStatistics
}

func (hjo *HashJoinOperator) Open(ctx *ExecutionContext) error {
    if err := hjo.BasePhysicalOperator.Open(ctx); err != nil {
        return err
    }
    
    // Initialize hash table
    estimatedBuildSize := hjo.estimateBuildSideSize()
    hjo.hashTable = NewHashTable(estimatedBuildSize)
    
    // Build phase statistics
    hjo.buildStats = NewBuildPhaseStatistics()
    hjo.probeStats = NewProbePhaseStatistics()
    
    return nil
}

func (hjo *HashJoinOperator) Next() (*Tuple, error) {
    // Build phase: populate hash table with left (build) side
    if !hjo.buildCompleted {
        if err := hjo.buildHashTable(); err != nil {
            return nil, fmt.Errorf("hash table build failed: %w", err)
        }
        hjo.buildCompleted = true
        hjo.probePhase = true
    }
    
    // Probe phase: probe with right side tuples
    return hjo.probeHashTable()
}

func (hjo *HashJoinOperator) buildHashTable() error {
    buildStart := time.Now()
    defer func() {
        hjo.buildStats.BuildTime = time.Since(buildStart)
    }()
    
    for {
        buildTuple, err := hjo.leftChild.Next()
        if err != nil {
            if err == io.EOF {
                break // Build phase complete
            }
            return err
        }
        
        // Extract join key from build tuple
        joinKey, err := hjo.extractJoinKey(buildTuple, hjo.leftJoinKeys)
        if err != nil {
            return fmt.Errorf("join key extraction failed: %w", err)
        }
        
        // Add to hash table
        hjo.hashTable.Insert(joinKey, buildTuple)
        hjo.buildStats.IncrementBuildTuples()
    }
    
    hjo.buildStats.HashTableSize = hjo.hashTable.Size()
    hjo.buildStats.MemoryUsage = hjo.hashTable.MemoryUsage()
    
    return nil
}

func (hjo *HashJoinOperator) probeHashTable() (*Tuple, error) {
    for {
        // If no current matches, get next probe tuple
        if hjo.currentMatchIndex >= len(hjo.matchingBuildTuples) {
            probeTuple, err := hjo.rightChild.Next()
            if err != nil {
                return nil, err // EOF or error
            }
            
            // Extract join key from probe tuple
            joinKey, err := hjo.extractJoinKey(probeTuple, hjo.rightJoinKeys)
            if err != nil {
                return nil, fmt.Errorf("probe key extraction failed: %w", err)
            }
            
            // Find matching build tuples
            hjo.matchingBuildTuples = hjo.hashTable.Lookup(joinKey)
            hjo.currentProbeTuple = probeTuple
            hjo.currentMatchIndex = 0
            hjo.probeStats.IncrementProbeTuples()
        }
        
        // Process current matches
        if hjo.currentMatchIndex < len(hjo.matchingBuildTuples) {
            buildTuple := hjo.matchingBuildTuples[hjo.currentMatchIndex].Tuple
            hjo.currentMatchIndex++
            
            // Create joined tuple
            joinedTuple := hjo.createJoinedTuple(buildTuple, hjo.currentProbeTuple)
            
            // Apply additional join conditions if present
            if hjo.joinCondition != nil {
                result, err := hjo.joinCondition.Evaluate(joinedTuple)
                if err != nil {
                    return nil, fmt.Errorf("join condition evaluation failed: %w", err)
                }
                if !result.(bool) {
                    continue // Skip non-matching tuple
                }
            }
            
            hjo.probeStats.IncrementJoinedTuples()
            return joinedTuple, nil
        }
        
        // No more matches for current probe tuple
        if hjo.joinType == LeftOuterJoin && len(hjo.matchingBuildTuples) == 0 {
            // Emit tuple with nulls for right side
            joinedTuple := hjo.createLeftOuterJoinTuple(hjo.currentProbeTuple)
            hjo.currentMatchIndex = len(hjo.matchingBuildTuples) // Force next probe tuple
            return joinedTuple, nil
        }
        
        // Reset for next probe tuple
        hjo.matchingBuildTuples = nil
        hjo.currentMatchIndex = 0
    }
}

// Nested loop join operator
type NestedLoopJoinOperator struct {
    BasePhysicalOperator
    
    // Join configuration
    leftChild          PhysicalOperator
    rightChild         PhysicalOperator
    joinType           JoinType
    joinCondition      Expression
    
    // Execution state
    currentLeftTuple   *Tuple
    rightChildReset    bool
    leftExhausted      bool
    
    // Statistics
    nlStats            *NestedLoopStatistics
}
```

#### Aggregation Operators
```go
// Hash aggregation operator
type HashAggregationOperator struct {
    BasePhysicalOperator
    
    // Aggregation configuration
    child              PhysicalOperator
    groupByExpressions []Expression
    aggregateFunctions []AggregateFunction
    
    // Hash table for grouping
    groupTable         *GroupHashTable
    aggregateStates    map[GroupKey]*AggregateState
    
    // Execution state
    buildCompleted     bool
    outputIterator     GroupIterator
    
    // Statistics
    aggStats           *AggregationStatistics
}

func (hao *HashAggregationOperator) Next() (*Tuple, error) {
    // Build phase: consume all input and compute aggregates
    if !hao.buildCompleted {
        if err := hao.buildAggregates(); err != nil {
            return nil, fmt.Errorf("aggregation build failed: %w", err)
        }
        hao.buildCompleted = true
        hao.outputIterator = hao.groupTable.Iterator()
    }
    
    // Output phase: emit aggregated results
    return hao.outputIterator.Next()
}

func (hao *HashAggregationOperator) buildAggregates() error {
    for {
        inputTuple, err := hao.child.Next()
        if err != nil {
            if err == io.EOF {
                break // All input consumed
            }
            return err
        }
        
        // Extract group key
        groupKey, err := hao.extractGroupKey(inputTuple)
        if err != nil {
            return fmt.Errorf("group key extraction failed: %w", err)
        }
        
        // Get or create aggregate state for this group
        aggState, exists := hao.aggregateStates[groupKey]
        if !exists {
            aggState = hao.createAggregateState()
            hao.aggregateStates[groupKey] = aggState
        }
        
        // Update aggregate functions
        for i, aggFunc := range hao.aggregateFunctions {
            if err := aggFunc.Update(aggState.States[i], inputTuple); err != nil {
                return fmt.Errorf("aggregate update failed: %w", err)
            }
        }
        
        hao.aggStats.IncrementInputTuples()
    }
    
    hao.aggStats.GroupCount = int64(len(hao.aggregateStates))
    return nil
}

// Sort-based aggregation operator
type SortAggregationOperator struct {
    BasePhysicalOperator
    
    // Aggregation configuration
    child              PhysicalOperator
    groupByExpressions []Expression
    aggregateFunctions []AggregateFunction
    
    // Sort-based processing
    currentGroup       GroupKey
    currentAggState    *AggregateState
    nextTuple          *Tuple
    groupComplete      bool
    
    // Statistics
    sortAggStats       *SortAggregationStatistics
}
```

### 3. Execution Context
Manages execution state and resources for a query.

```go
type ExecutionContext struct {
    // Query identification
    QueryID            QueryID
    SessionID          SessionID
    TransactionID      TransactionID
    
    // Execution configuration
    Config             *ExecutionConfig
    Timeout            time.Duration
    Priority           ExecutionPriority
    
    // Resource managers
    MemoryManager      *ExecutionMemoryManager
    StorageManager     StorageManager
    TransactionManager TransactionManager
    
    // Execution state
    StartTime          time.Time
    Status             ExecutionStatus
    CancellationToken  context.CancelFunc
    
    // Statistics collection
    Statistics         *ExecutionStatistics
    OperatorStats      map[OperatorID]*OperatorStatistics
    
    // Temporary resources
    TempSpillFiles     []string
    SpillDirectory     string
    
    // Concurrency control
    mutex              sync.RWMutex
}

type ExecutionConfig struct {
    // Memory limits
    MaxMemoryUsage     int64
    SpillThreshold     int64
    TempDirectory      string
    
    // Parallelism
    MaxParallelWorkers int
    EnableParallelism  bool
    
    // Performance tuning
    BatchSize          int
    PrefetchSize       int
    
    // Debugging
    EnableProfiling    bool
    ProfileSampleRate  float64
    LogLevel           LogLevel
}
```

### 4. Memory Management
Manages memory allocation and spilling for execution operators.

```go
type ExecutionMemoryManager struct {
    // Memory budgets
    totalMemoryBudget   int64
    currentMemoryUsage  int64
    operatorBudgets     map[OperatorID]int64
    
    // Spill management
    spillManager        *SpillManager
    spillThreshold      int64
    spillEnabled        bool
    
    // Memory pools
    tuplePool           *sync.Pool
    bufferPool          *sync.Pool
    
    // Monitoring
    memoryPressure      MemoryPressureLevel
    allocationStats     *MemoryAllocationStats
    
    mutex               sync.RWMutex
}

func (emm *ExecutionMemoryManager) AllocateMemory(
    operatorID OperatorID, size int64) (*MemoryAllocation, error) {
    
    emm.mutex.Lock()
    defer emm.mutex.Unlock()
    
    // Check if allocation would exceed budget
    if emm.currentMemoryUsage + size > emm.totalMemoryBudget {
        // Try to free memory through spilling
        if err := emm.triggerSpilling(size); err != nil {
            return nil, fmt.Errorf("memory allocation failed: %w", err)
        }
    }
    
    // Allocate memory
    allocation := &MemoryAllocation{
        OperatorID: operatorID,
        Size:       size,
        Address:    emm.allocateRawMemory(size),
        Timestamp:  time.Now(),
    }
    
    emm.currentMemoryUsage += size
    emm.allocationStats.RecordAllocation(size)
    
    return allocation, nil
}

type SpillManager struct {
    // Spill configuration
    spillDirectory     string
    maxSpillFiles      int
    spillFilePrefix    string
    
    // Active spill files
    activeSpillFiles   map[OperatorID][]SpillFile
    spillFileCounter   int64
    
    // Spill statistics
    spillStats         *SpillStatistics
    
    mutex              sync.RWMutex
}

func (sm *SpillManager) SpillOperatorData(
    operatorID OperatorID, data []Tuple) (*SpillFile, error) {
    
    sm.mutex.Lock()
    defer sm.mutex.Unlock()
    
    // Generate unique spill file name
    spillFileName := fmt.Sprintf("%s_%s_%d.spill", 
        sm.spillFilePrefix, operatorID, atomic.AddInt64(&sm.spillFileCounter, 1))
    
    spillPath := filepath.Join(sm.spillDirectory, spillFileName)
    
    // Write data to spill file
    file, err := os.Create(spillPath)
    if err != nil {
        return nil, fmt.Errorf("failed to create spill file: %w", err)
    }
    defer file.Close()
    
    encoder := NewTupleEncoder(file)
    for _, tuple := range data {
        if err := encoder.Encode(tuple); err != nil {
            os.Remove(spillPath)
            return nil, fmt.Errorf("failed to encode tuple: %w", err)
        }
    }
    
    spillFile := &SpillFile{
        Path:        spillPath,
        OperatorID:  operatorID,
        TupleCount:  int64(len(data)),
        Size:        encoder.BytesWritten(),
        CreatedAt:   time.Now(),
    }
    
    // Track spill file
    if sm.activeSpillFiles[operatorID] == nil {
        sm.activeSpillFiles[operatorID] = make([]SpillFile, 0)
    }
    sm.activeSpillFiles[operatorID] = append(sm.activeSpillFiles[operatorID], *spillFile)
    
    sm.spillStats.RecordSpillOperation(spillFile.Size)
    
    return spillFile, nil
}
```

### 5. Task Scheduler
Manages parallel execution of operators and tasks.

```go
type TaskScheduler struct {
    // Worker pool
    workerPool         *WorkerPool
    taskQueue          chan Task
    maxWorkers         int
    activeWorkers      int32
    
    // Task management
    activeTasks        map[TaskID]*TaskExecution
    taskDependencies   *DependencyGraph
    
    // Scheduling policies
    scheduler          SchedulingPolicy
    priorityQueue      *PriorityTaskQueue
    
    // Resource allocation
    resourceAllocator  *ResourceAllocator
    
    mutex              sync.RWMutex
}

type Task interface {
    // Task identification
    ID() TaskID
    Type() TaskType
    Priority() TaskPriority
    
    // Dependencies
    Dependencies() []TaskID
    Resources() ResourceRequirements
    
    // Execution
    Execute(ctx *ExecutionContext, worker *Worker) error
    Cancel() error
    
    // Status
    Status() TaskStatus
    Progress() float64
}

// Parallel scan task
type ParallelScanTask struct {
    taskID             TaskID
    tableName          string
    scanRange          ScanRange
    filter             Expression
    projection         []ColumnExpression
    outputChannel      chan<- *Tuple
    
    // Execution state
    status             TaskStatus
    progress           float64
    errorChan          chan error
    
    // Statistics
    scannedRows        int64
    producedRows       int64
}

func (pst *ParallelScanTask) Execute(ctx *ExecutionContext, worker *Worker) error {
    // Get storage iterator for scan range
    iterator, err := ctx.StorageManager.NewRangeIterator(pst.tableName, pst.scanRange)
    if err != nil {
        return fmt.Errorf("failed to create range iterator: %w", err)
    }
    defer iterator.Close()
    
    batch := make([]*Tuple, 0, ctx.Config.BatchSize)
    
    for {
        tuple, err := iterator.Next()
        if err != nil {
            if err == io.EOF {
                break // Scan complete
            }
            return fmt.Errorf("scan error: %w", err)
        }
        
        pst.scannedRows++
        
        // Apply filter
        if pst.filter != nil {
            result, err := pst.filter.Evaluate(tuple)
            if err != nil {
                return fmt.Errorf("filter evaluation error: %w", err)
            }
            if !result.(bool) {
                continue
            }
        }
        
        // Apply projection
        if len(pst.projection) > 0 {
            tuple, err = pst.applyProjection(tuple)
            if err != nil {
                return fmt.Errorf("projection error: %w", err)
            }
        }
        
        batch = append(batch, tuple)
        pst.producedRows++
        
        // Send batch when full
        if len(batch) >= ctx.Config.BatchSize {
            if err := pst.sendBatch(batch); err != nil {
                return err
            }
            batch = batch[:0]
        }
        
        // Update progress
        pst.updateProgress()
        
        // Check for cancellation
        select {
        case <-ctx.Done():
            return ctx.Err()
        default:
        }
    }
    
    // Send remaining tuples
    if len(batch) > 0 {
        return pst.sendBatch(batch)
    }
    
    return nil
}

type WorkerPool struct {
    workers            []*Worker
    availableWorkers   chan *Worker
    maxWorkers         int
    currentWorkers     int32
    
    // Worker lifecycle management
    workerFactory      WorkerFactory
    workerStats        map[WorkerID]*WorkerStatistics
    
    mutex              sync.RWMutex
}

type Worker struct {
    id                 WorkerID
    status             WorkerStatus
    currentTask        Task
    
    // Execution context
    context            *ExecutionContext
    memoryLimit        int64
    
    // Statistics
    tasksCompleted     int64
    totalExecutionTime time.Duration
    errorCount         int64
    
    // Cancellation
    cancel             context.CancelFunc
    
    mutex              sync.RWMutex
}
```

## Architecture Layers

### Layer 1: Execution Interface
- **ExecutionEngine**: Main entry point for query execution
- **QueryExecutor**: High-level query execution coordinator
- **ResultSetBuilder**: Constructs and formats result sets

### Layer 2: Operator Execution
- **PhysicalOperators**: Implements all physical operators
- **OperatorFactory**: Creates operator instances from plans
- **OperatorRegistry**: Manages available operator implementations

### Layer 3: Resource Management
- **MemoryManager**: Manages memory allocation and spilling
- **TaskScheduler**: Coordinates parallel execution
- **ResourceAllocator**: Allocates CPU, memory, and I/O resources

### Layer 4: Storage Interface
- **StorageInterface**: Abstraction over storage engines
- **TransactionInterface**: Integration with transaction management
- **IndexInterface**: Access to index structures

## Key Architectural Decisions

### 1. Volcano Iterator Model
**Decision**: Use volcano-style iterator model for operator execution.

**Rationale**:
- Simple and composable operator interface
- Natural support for pipelining
- Easy to implement and debug
- Compatible with lazy evaluation

**Implementation**:
```go
type OperatorIterator interface {
    Open(ctx *ExecutionContext) error
    Next() (*Tuple, error)
    Close() error
}
```

### 2. Pull-Based Execution
**Decision**: Implement pull-based execution model where parent operators pull data from children.

**Benefits**:
- Natural flow control and backpressure
- Memory-efficient execution
- Easy to implement blocking operators
- Predictable resource usage

### 3. Hybrid Memory Management
**Decision**: Combine in-memory processing with spilling for large datasets.

**Strategy**:
- Track memory usage per operator
- Spill to disk when memory limits are reached
- Use memory pools for frequent allocations
- Implement efficient spill/restore mechanisms

### 4. Parallel Execution Support
**Decision**: Support both intra-operator and inter-operator parallelism.

**Approaches**:
- **Intra-operator**: Parallel scans, parallel hash joins
- **Inter-operator**: Pipeline parallelism between operators
- **Exchange operators**: For data redistribution in parallel plans

## Integration Points

### 1. Optimizer Integration
```go
type OptimizerExecutorInterface struct {
    // Plan execution
    ExecutePlan(ctx context.Context, plan PhysicalPlan) (*ResultSet, error)
    
    // Cost feedback
    GetActualCosts(queryID QueryID) (*ActualCostMetrics, error)
    GetCardinalities(queryID QueryID) (map[OperatorID]int64, error)
    
    // Plan validation
    ValidatePlan(plan PhysicalPlan) error
    EstimateResources(plan PhysicalPlan) (*ResourceEstimate, error)
}
```

### 2. Storage Integration
```go
type StorageExecutorInterface struct {
    // Table access
    NewTableIterator(tableName string, scanRanges []ScanRange) (TableIterator, error)
    NewIndexIterator(tableName, indexName string, keyRange KeyRange) (IndexIterator, error)
    
    // Transaction support
    BeginTransaction(isolation IsolationLevel) (TransactionID, error)
    CommitTransaction(txnID TransactionID) error
    RollbackTransaction(txnID TransactionID) error
    
    // Locking
    AcquireLocks(txnID TransactionID, locks []LockRequest) error
    ReleaseLocks(txnID TransactionID) error
}
```

### 3. Transaction Manager Integration
```go
type TransactionExecutorInterface struct {
    // Visibility
    IsVisible(tuple *Tuple, txnID TransactionID) (bool, error)
    GetSnapshot(txnID TransactionID) (*Snapshot, error)
    
    // Concurrency control
    CheckConflicts(txnID TransactionID, operation Operation) error
    RecordOperation(txnID TransactionID, operation Operation) error
}
```

## Performance Optimizations

### 1. Vectorized Execution
```go
type VectorizedOperator interface {
    OpenVectorized(ctx *ExecutionContext) error
    NextBatch(batch *TupleBatch) error
    CloseVectorized() error
}

type TupleBatch struct {
    Tuples     []*Tuple
    Size       int
    Capacity   int
    Schema     *Schema
}
```

### 2. Code Generation
```go
type CodeGenerator struct {
    templateEngine  *TemplateEngine
    compiler        *JITCompiler
    codeCache       map[OperatorSignature]*CompiledOperator
}

func (cg *CodeGenerator) GenerateOperator(
    operator PhysicalOperator, 
    schema *Schema) (*CompiledOperator, error) {
    
    // Generate specialized code for operator
    code := cg.generateOperatorCode(operator, schema)
    
    // Compile to native code
    compiled, err := cg.compiler.Compile(code)
    if err != nil {
        return nil, fmt.Errorf("compilation failed: %w", err)
    }
    
    return compiled, nil
}
```

### 3. Adaptive Execution
```go
type AdaptiveExecutionEngine struct {
    // Runtime statistics
    runtimeStats       *RuntimeStatistics
    adaptationTriggers *AdaptationTriggers
    
    // Plan modification
    planModifier       *PlanModifier
    operatorSwitcher   *OperatorSwitcher
}

func (aee *AdaptiveExecutionEngine) MonitorExecution(ctx *ExecutionContext) {
    // Monitor for adaptation opportunities
    go func() {
        ticker := time.NewTicker(100 * time.Millisecond)
        defer ticker.Stop()
        
        for {
            select {
            case <-ticker.C:
                aee.checkForAdaptations(ctx)
            case <-ctx.Done():
                return
            }
        }
    }()
}
```

## Error Handling and Recovery

### 1. Error Propagation
```go
type ExecutionError struct {
    OperatorID    OperatorID
    ErrorType     ErrorType
    Message       string
    Cause         error
    Timestamp     time.Time
    Recoverable   bool
}

func (ee *ExecutionError) Error() string {
    return fmt.Sprintf("[%s] %s: %s", ee.OperatorID, ee.ErrorType, ee.Message)
}
```

### 2. Checkpoint and Recovery
```go
type CheckpointManager struct {
    checkpointStore   CheckpointStore
    recoveryManager   *RecoveryManager
    checkpointPolicy  *CheckpointPolicy
}

type ExecutionCheckpoint struct {
    QueryID           QueryID
    Timestamp         time.Time
    OperatorStates    map[OperatorID]*OperatorState
    MemorySnapshot    *MemorySnapshot
    ResourceState     *ResourceState
}
```

## Monitoring and Observability

### 1. Execution Statistics
```go
type ExecutionStatistics struct {
    // Timing
    StartTime         time.Time
    EndTime           time.Time
    ExecutionTime     time.Duration
    
    // Resource usage
    PeakMemoryUsage   int64
    TotalCPUTime      time.Duration
    IOOperations      int64
    NetworkBytes      int64
    
    // Operator statistics
    OperatorStats     map[OperatorID]*OperatorStatistics
    
    // Error tracking
    Errors            []ExecutionError
    Warnings          []ExecutionWarning
}

type OperatorStatistics struct {
    // Tuple processing
    TuplesProcessed   int64
    TuplesProduced    int64
    ProcessingTime    time.Duration
    
    // Resource usage
    MemoryUsage       int64
    SpillOperations   int64
    IOReads           int64
    IOWrites          int64
    
    // Performance metrics
    ThroughputTuples  float64
    LatencyPerTuple   time.Duration
    EfficiencyRatio   float64
}
```

### 2. Runtime Profiling
```go
type ExecutionProfiler struct {
    enabled           bool
    sampleRate        float64
    profiles          map[QueryID]*ExecutionProfile
    profilerMutex     sync.RWMutex
}

type ExecutionProfile struct {
    CPUProfile        *CPUProfile
    MemoryProfile     *MemoryProfile
    IOProfile         *IOProfile
    LockProfile       *LockProfile
}
```

## Scalability and Distribution

### 1. Distributed Execution
```go
type DistributedExecutionEngine struct {
    localEngine       *ExecutionEngine
    remoteExecutors   map[NodeID]*RemoteExecutor
    exchangeManager   *ExchangeManager
    
    // Distribution strategy
    partitioner       *DataPartitioner
    loadBalancer      *LoadBalancer
}

type ExchangeOperator struct {
    BasePhysicalOperator
    
    // Exchange configuration
    exchangeType      ExchangeType  // GATHER, DISTRIBUTE, REPARTITION
    partitionFunction PartitionFunction
    targetNodes       []NodeID
    
    // Network communication
    sender            *DataSender
    receiver          *DataReceiver
    compressionCodec  CompressionCodec
}
```

### 2. Cloud-Native Features
```go
type CloudExecutionEngine struct {
    // Resource elasticity
    autoScaler        *AutoScaler
    resourceMonitor   *ResourceMonitor
    
    // Storage abstraction
    storageAdapters   map[StorageType]StorageAdapter
    
    // Multi-tenancy
    tenantManager     *TenantManager
    resourceIsolator  *ResourceIsolator
}
```

This architecture provides a robust, scalable, and efficient query execution engine capable of handling complex queries across various workloads while maintaining high performance and reliability.